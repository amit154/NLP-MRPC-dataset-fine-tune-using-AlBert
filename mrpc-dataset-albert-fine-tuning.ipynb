{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\n",
      "\u001b[K     |████████████████████████████████| 186 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.3.0)\n",
      "Collecting tqdm<4.50.0,>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 3.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.5)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.2\n",
      "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.0.2->datasets) (3.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2020.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.55.1\n",
      "    Uninstalling tqdm-4.55.1:\n",
      "      Successfully uninstalled tqdm-4.55.1\n",
      "Successfully installed datasets-1.4.1 huggingface-hub-0.0.2 tqdm-4.49.0 xxhash-2.0.0\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "!pip install datasets\n",
    "!pip install -q tf-models-official==2.3.0\n",
    "#gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
    "#tf.io.gfile.listdir(gs_folder_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "from official.nlp import albert,bert\n",
    "\n",
    "# Load the required submodules\n",
    "import official.nlp.optimization\n",
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.run_classifier\n",
    "import official.nlp.bert.tokenization\n",
    "import official.nlp.data.classifier_data_lib\n",
    "import official.nlp.modeling.losses\n",
    "import official.nlp.modeling.models\n",
    "import official.nlp.modeling.networks\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['30k-clean.model',\n",
       " '30k-clean.vocab',\n",
       " 'albert_config.json',\n",
       " 'bert_model.ckpt.data-00000-of-00001',\n",
       " 'bert_model.ckpt.index']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_folder_bert = \"gs://cloud-tpu-checkpoints/albert/checkpoints/albert_v2_base\"\n",
    "tf.io.gfile.listdir(gs_folder_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the mrpc dataset\n",
    "original = load_dataset('glue', 'mrpc')\n",
    "#original, info = tfds.load('glue/mrpc', with_info=True,batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3668, 4)\n",
      "(408, 4)\n",
      "(1725, 4)\n"
     ]
    }
   ],
   "source": [
    "# Spliting the datasets into test train and validation\n",
    "#original = dataset['train'].train_test_split(test_size=0.1, seed=1)\n",
    "train = pd.DataFrame(original['train'])  \n",
    "val = pd.DataFrame(original['validation'])\n",
    "test = pd.DataFrame(original['test']) \n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "#Setup Tokeniser\n",
    "bert_model=\"albert-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
    "print(len(tokenizer.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sentences\n",
    "def encode_sentence(s, tokenizer):\n",
    "    tokens = list(tokenizer.tokenize(s))\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(glue_dict, tokenizer):\n",
    "    num_examples = len(glue_dict[\"sentence1\"])\n",
    "    sentence1 = tf.ragged.constant([encode_sentence(s, tokenizer) for s in np.array(glue_dict[\"sentence1\"])])\n",
    "    sentence2 = tf.ragged.constant([encode_sentence(s, tokenizer) for s in np.array(glue_dict[\"sentence2\"])])\n",
    "\n",
    "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
    "    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
    "\n",
    "    input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "    type_cls = tf.zeros_like(cls)\n",
    "    type_s1 = tf.zeros_like(sentence1)\n",
    "    type_s2 = tf.ones_like(sentence2)\n",
    "    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
    "\n",
    "    inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "    return inputs\n",
    "\n",
    "mrpc_train = bert_encode(train, tokenizer)\n",
    "mrpc_train_labels = train['label']\n",
    "\n",
    "mrpc_validation = bert_encode(val, tokenizer)\n",
    "mrpc_validation_labels = val['label']\n",
    "\n",
    "mrpc_test = bert_encode(test, tokenizer)\n",
    "mrpc_test_labels  = test['label']\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_probs_dropout_prob': 0,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0,\n",
       " 'embedding_size': 128,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'max_position_embeddings': 512,\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'num_hidden_groups': 1,\n",
       " 'net_structure_type': 0,\n",
       " 'gap_size': 0,\n",
       " 'num_memory_blocks': 0,\n",
       " 'inner_group_num': 1,\n",
       " 'down_scale_factor': 1,\n",
       " 'type_vocab_size': 2,\n",
       " 'vocab_size': 30000}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Configuration\n",
    "import json\n",
    "\n",
    "albert_config_file = os.path.join(gs_folder_bert, \"albert_config.json\")\n",
    "config_dict = json.loads(tf.io.gfile.GFile(albert_config_file).read())\n",
    "\n",
    "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
    "\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasifier and encoder\n",
    "bert_classifier, bert_encoder = bert.bert_models.classifier_model(\n",
    "    bert_config, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up epochs and steps\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "eval_batch_size = 16\n",
    "\n",
    "train_data_size = len(mrpc_train_labels)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = nlp.optimization.create_optimizer(optimizer_type='adamw',\n",
    "   init_lr =  2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "230/230 [==============================] - 89s 326ms/step - loss: 0.6438 - accuracy: 0.6706 - val_loss: 0.6344 - val_accuracy: 0.6838\n",
      "Epoch 2/10\n",
      "230/230 [==============================] - 74s 321ms/step - loss: 0.6264 - accuracy: 0.6644 - val_loss: 0.6198 - val_accuracy: 0.6912\n",
      "Epoch 3/10\n",
      "230/230 [==============================] - 74s 321ms/step - loss: 0.5784 - accuracy: 0.7199 - val_loss: 0.6493 - val_accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "230/230 [==============================] - 74s 320ms/step - loss: 0.5289 - accuracy: 0.7339 - val_loss: 0.6686 - val_accuracy: 0.6176\n",
      "Epoch 5/10\n",
      "230/230 [==============================] - 74s 320ms/step - loss: 0.4869 - accuracy: 0.7745 - val_loss: 0.7131 - val_accuracy: 0.6250\n",
      "Epoch 6/10\n",
      "230/230 [==============================] - 74s 321ms/step - loss: 0.4097 - accuracy: 0.8199 - val_loss: 0.7269 - val_accuracy: 0.6667\n",
      "Epoch 7/10\n",
      "230/230 [==============================] - 74s 321ms/step - loss: 0.3734 - accuracy: 0.8369 - val_loss: 0.8012 - val_accuracy: 0.6569\n",
      "Epoch 8/10\n",
      "230/230 [==============================] - 74s 321ms/step - loss: 0.3275 - accuracy: 0.8660 - val_loss: 0.8335 - val_accuracy: 0.6569\n",
      "Epoch 9/10\n",
      "230/230 [==============================] - 74s 321ms/step - loss: 0.2815 - accuracy: 0.8927 - val_loss: 0.9211 - val_accuracy: 0.6691\n",
      "Epoch 10/10\n",
      "230/230 [==============================] - 74s 321ms/step - loss: 0.2756 - accuracy: 0.8942 - val_loss: 0.9695 - val_accuracy: 0.6495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f835ea9d950>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "bert_classifier.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics)\n",
    "\n",
    "bert_classifier.fit(\n",
    "      mrpc_train, mrpc_train_labels,\n",
    "      validation_data=(mrpc_validation, mrpc_validation_labels),\n",
    "      batch_size=16,\n",
    "      epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir='./SavedModel.pb'\n",
    "tf.saved_model.save(bert_classifier, export_dir=export_dir)\n",
    "\n",
    "# Data is stored as .pb file by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking user input and storing the tokenised tensors in a textfile to load in Java later\n",
    "#sent1 = input()\n",
    "#sent2 = input()\n",
    "\n",
    "sent1  = \"They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .\"\n",
    "sent2 = \"On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\"\n",
    "\n",
    "my_examples = bert_encode(\n",
    "    glue_dict = {\n",
    "        'sentence1':[str(sent1)],\n",
    "        'sentence2':[str(sent2)]\n",
    "    },\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "input_word_ids = my_examples['input_word_ids'][0].numpy().astype(int)\n",
    "input_mask = my_examples['input_mask'][0].numpy().astype(int)\n",
    "input_type_ids  = my_examples['input_type_ids'][0].numpy().astype(int)\n",
    "np.savetxt('user_input.txt', (input_word_ids.astype(int),input_mask.astype(int),input_type_ids.astype(int)),fmt='%d') \n",
    "\n",
    "# the \"user_input.txt\" file is later stored in java folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir './SavedModel/' --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
    "\n",
    "signature_def['__saved_model_init_op']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['__saved_model_init_op'] tensor_info:\n",
    "        dtype: DT_INVALID\n",
    "        shape: unknown_rank\n",
    "        name: NoOp\n",
    "  Method name is: \n",
    "\n",
    "signature_def['serving_default']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "    inputs['input_mask'] tensor_info:\n",
    "        dtype: DT_INT32\n",
    "        shape: (-1, -1)\n",
    "        name: serving_default_input_mask:0\n",
    "    inputs['input_type_ids'] tensor_info:\n",
    "        dtype: DT_INT32\n",
    "        shape: (-1, -1)\n",
    "        name: serving_default_input_type_ids:0\n",
    "    inputs['input_word_ids'] tensor_info:\n",
    "        dtype: DT_INT32\n",
    "        shape: (-1, -1)\n",
    "        name: serving_default_input_word_ids:0\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['classification'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 2)\n",
    "        name: StatefulPartitionedCall:0\n",
    "  Method name is: tensorflow/serving/predict\n",
    "\n",
    "Defined Functions:\n",
    "  Function Name: '__call__'\n",
    "    Option #1\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          DType: list\n",
    "          Value: [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: False\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "    Option #2\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          DType: list\n",
    "          Value: [TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/2')]\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: False\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "    Option #3\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          DType: list\n",
    "          Value: [TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/2')]\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: True\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "    Option #4\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          DType: list\n",
    "          Value: [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: True\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "\n",
    "  Function Name: '_default_save_signature'\n",
    "    Option #1\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          DType: list\n",
    "          Value: [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]\n",
    "\n",
    "  Function Name: 'call_and_return_all_conditional_losses'\n",
    "    Option #1\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          DType: list\n",
    "          Value: [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: False\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "    Option #2\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          DType: list\n",
    "          Value: [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: True\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "    Option #3\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          DType: list\n",
    "          Value: [TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/2')]\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: False\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "    Option #4\n",
    "      Callable with:\n",
    "        Argument #1\n",
    "          DType: list\n",
    "          Value: [TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/2')]\n",
    "        Argument #2\n",
    "          DType: bool\n",
    "          Value: True\n",
    "        Argument #3\n",
    "          DType: NoneType\n",
    "          Value: None\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
